# Definitions

## Inductive inference / reasoning
"The process of reaching a general conclusion from specific examples." [1]

Examples are generalized into a model that is then used to make predictions on new, previously unseen inputs.

Examples can be all positive, or a mixture of positive and negative. Negative examples constrain the set of models that are consistent with the examples [1].

## Inductive Learning Hypothesis
"Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples." [1]

## Inductive bias
"Explicit or implicit assumption(s) about what kind of model is wanted." [1]

## Concept Learning

### Classical view (philosophy)
"a process of abstraction, data compression, simplification, and summarization" [2]

### Rule-based
- Most have been powered by heuristics, not statistics [2].

### Prototype
Assumptions [2]:
- "people abstract out the central tendency (or prototype) of the examples experienced and use this as a basis for their categorization decisions." 
- "people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples."
	- "people do not categorize based on a list of things that all correspond to a definition, but rather on a hierarchical inventory based on semantic similarity to the central example(s)."

### Exemplar
Assumptions [2]:
- people "store examples verbatim."

Exemplar models require two measures [2]:
- similarity between exemplars
- rules to determine group membership

### Explanation-based generalization

### Bayesian

## Grammar

- of human languages [3]: 
	- "The grammars of human languages produce hierarchical tree structures." 
	- "Some linguists argue that human languages are also capable of infinite recursion (see context-free grammar)."
		- *many indistinguishable possible grammars for producing a finite set of observations*: "For any given set of sentences generated by a hierarchical grammar capable of infinite recursion, there are an indefinite number of grammars that could have produced the same data."
			- In theory, the correct grammar is unlearnable
				- Proof by E. Mark Gold "showed that any formal language that has hierarchical structure capable of infinite recursion is unlearnable from positive evidence alone."
				- "it is impossible to formulate a procedure that will discover with certainty the correct grammar given any arbitrary sequence of positive data in which each utterance occurs at least once."
			- In practice, the correct grammar, or a good approximation, is learnable
				- Gold's proof "does not preclude arriving at the correct grammar using typical input sequences rather than particularly malicious sequences or arrive at an almost perfect approximation to the correct grammar."
				- Proposition: "under very mild assumptions (ergodicity and stationarity), the probability of producing a sequence that renders language learning impossible is in fact zero."

### Context-free grammar

### Grammar induction

## Version space

### learning

### algebra

## Poverty of the Stimulus



[1] http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/2_inference.html
[2] https://en.wikipedia.org/wiki/Concept_learning#Modern_psychological_theories
[3] https://en.wikipedia.org/wiki/Poverty_of_the_stimulus